#+STARTUP: inlineimages
#+TITLE: SpeechState
[[https://www.npmjs.com/package/speechstate][file:https://badge.fury.io/js/speechstate.svg]]


Free browser-based spoken dialogue system. Based on [[https://github.com/statelyai/xstate][XState]]. 

* SDK

** Spawn SpeechState
#+begin_src typescript
  import { speechstate } from "speechstate";

  // define your the context of your statechart as follows
  context: ({ spawn }) => ({
    spstRef: spawn(speechstate, { input: settings }),
  }),
#+end_src

#+begin_src typescript
  interface AzureCredentials {
    endpoint: string;
    key: string;
  }

  interface Settings {
    locale?: string;
    azureCredentials: string | AzureCredentials;
    azureRegion: string;
    azureLanguageCredentials?: AzureLanguageCredentials;
    asrDefaultCompleteTimeout?: number;
    asrDefaultNoInputTimeout?: number;
    speechRecognitionEndpointId?: string;
    ttsDefaultVoice?: string;
    ttsLexicon?: string;
  }
#+end_src


** Events
Example action:
#+begin_src typescript
  ({ context }) =>
    context.spstRef.send({
      type: "SPEAK",
      value: { utterance: "Hello world", voice: "en-GB-RyanNeural" },
    });
#+end_src

*** DM to SpeechState
- ~{ type: "PREPARE" }~  
- ~{ type: "SPEAK"; value: Agenda }~ 
- ~{ type: "LISTEN"; value?: RecogniseParameters }~
- ~{ type: "CONTROL" }~
- ~{ type: "STOP" }~


*** SpeechState to DM
- ~{ type: "ASRTTS_READY" }~  
- ~{ type: "ASR_NOINPUT" }~
- ~{ type: "RECOGNISED"; value: Hypothesis[]; nluValue?: any }~
- ~{ type: "LISTEN_COMPLETE" }~
- ~{ type: "SPEAK_COMPLETE" }~ 
- ~{ type: "ASR_STARTED" }~ 
- ~{ type: "TTS_STARTED" }~ 
** Types:
#+begin_src typescript
  interface Hypothesis {
    utterance: string;
    confidence: number;
  }

  interface Agenda {
    utterance: string;
    voice?: string; // defaults to "en-US-DavisNeural"
    locale?: string;
    stream?: string;
    cache?: string;
    fillerDelay?: number;
    visemes?: boolean;
    audioURL?: string;
  }

  interface RecogniseParameters {
    noInputTimeout?: number;
    completeTimeout?: number;
    locale?: string;
    hints?: string[];
    nlu?: boolean | AzureLanguageCredentials;
  }

  interface AzureLanguageCredentials {
    endpoint: string;
    key: string;
    projectName: string;
    deploymentName: string;
  }
#+end_src


* How to run ~SpeechState~
** Create Azure account and enable speech services
1. Apply for free student credits
   https://azure.microsoft.com/en-us/free/students/. You should be
   able to login with your GU account.
2. Make sure that you are logged into the Azure portal (https://portal.azure.com/).
3. Create a *Resource group* (you can use search field):
   - Subscription: *Azure for students*
   - Resource group: any name
   - Region: *(Europe) North Europe*
4. Create a *Speech service*:
   - Name: any name
   - Subscription: *Azure for students*
   - Location: *(Europe) North Europe*
   - Pricing tier: *Free (F0)*
   - Resource group: group name from the previous step
5. Within your Speech Service go to: *Resourse management â†’ Keys and
   Endpoint* and copy your KEY 1.
* Sequence diagrams
#+begin_src plantuml :results output replace :file docs/diagrams/dm-speechstate.svg :exports results
  skinparam defaultFontName Helvetica
  participant       SpeechState       as SS
  hnote across: Initialization
  create SS
  DM -> SS : **spawn**
  DM -> SS : PREPARE
  SS --> DM : ASRTTS_READY
  hnote across: Speech Synthesis 
  DM -> SS : SPEAK
  activate SS
  SS --> DM : TTS_STARTED
  return SPEAK_COMPLETE
  hnote across: Speech Recognition
  DM -> SS : LISTEN
  activate SS
  SS --> DM : ASR_STARTED
  SS --> DM : RECOGNISED
  return LISTEN_COMPLETE
  DM -> SS : LISTEN
  activate SS
  SS --> DM : ASR_STARTED
  ... > noInputTimeout ...
  SS --> DM : ASR_NOINPUT
  return LISTEN_COMPLETE
#+end_src

#+RESULTS:
[[file:docs/diagrams/dm-speechstate.svg]]


