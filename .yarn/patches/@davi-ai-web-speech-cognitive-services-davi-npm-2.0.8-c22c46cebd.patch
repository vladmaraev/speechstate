diff --git a/dist/index.js b/dist/index.js
index 9a0e0a190f09c6eb75cd210e4ac16217043722e4..054da0128215d29df090fe5d9e485453cf0ddd4d 100644
--- a/dist/index.js
+++ b/dist/index.js
@@ -1,13 +1,17 @@
 var $8zHUo$microsoftcognitiveservicesspeechsdk = require("microsoft-cognitiveservices-speech-sdk");
 var $8zHUo$pdeferes5 = require("p-defer-es5");
 
-
 function $parcel$defineInteropFlag(a) {
-  Object.defineProperty(a, '__esModule', {value: true, configurable: true});
+  Object.defineProperty(a, "__esModule", { value: true, configurable: true });
 }
 
 function $parcel$export(e, n, v, s) {
-  Object.defineProperty(e, n, {get: v, set: s, enumerable: true, configurable: true});
+  Object.defineProperty(e, n, {
+    get: v,
+    set: s,
+    enumerable: true,
+    configurable: true,
+  });
 }
 
 function $parcel$interopDefault(a) {
@@ -16,144 +20,251 @@ function $parcel$interopDefault(a) {
 
 $parcel$defineInteropFlag(module.exports);
 
-$parcel$export(module.exports, "default", () => $882b6d93070905b3$export$2e2bcd8739ae039);
-$parcel$export(module.exports, "createSpeechRecognitionPonyfill", () => $87729c1c3277125d$export$7c06391dbd8298b0);
-$parcel$export(module.exports, "createSpeechSynthesisPonyfill", () => $0adf7fcdea541e7b$export$cf962f8b61871d5c);
-$parcel$export(module.exports, "fetchAuthorizationToken", () => $3e54160c801e3614$export$2e2bcd8739ae039);
-$parcel$export(module.exports, "SpeechSynthesisUtterance", () => $fd1e2557ea351c52$export$2e2bcd8739ae039);
-$parcel$export(module.exports, "SpeechSynthesis", () => $0adf7fcdea541e7b$export$1268b12b5ca510be);
-$parcel$export(module.exports, "SpeechGrammarList", () => $c213b3979e1323e8$export$2e2bcd8739ae039);
-/* eslint class-methods-use-this: "off" */ /* eslint complexity: ["error", 70] */ /* eslint no-await-in-loop: "off" */ /* eslint no-empty-function: "off" */ /* eslint no-magic-numbers: ["error", { "ignore": [0, 100, 150] }] */ function $e3680afa520905a3$export$2e2bcd8739ae039(array, extras) {
-    const map = {
+$parcel$export(
+  module.exports,
+  "default",
+  () => $882b6d93070905b3$export$2e2bcd8739ae039,
+);
+$parcel$export(
+  module.exports,
+  "createSpeechRecognitionPonyfill",
+  () => $87729c1c3277125d$export$7c06391dbd8298b0,
+);
+$parcel$export(
+  module.exports,
+  "createSpeechSynthesisPonyfill",
+  () => $0adf7fcdea541e7b$export$cf962f8b61871d5c,
+);
+$parcel$export(
+  module.exports,
+  "fetchAuthorizationToken",
+  () => $3e54160c801e3614$export$2e2bcd8739ae039,
+);
+$parcel$export(
+  module.exports,
+  "SpeechSynthesisUtterance",
+  () => $fd1e2557ea351c52$export$2e2bcd8739ae039,
+);
+$parcel$export(
+  module.exports,
+  "SpeechSynthesis",
+  () => $0adf7fcdea541e7b$export$1268b12b5ca510be,
+);
+$parcel$export(
+  module.exports,
+  "SpeechGrammarList",
+  () => $c213b3979e1323e8$export$2e2bcd8739ae039,
+);
+/* eslint class-methods-use-this: "off" */ /* eslint complexity: ["error", 70] */ /* eslint no-await-in-loop: "off" */ /* eslint no-empty-function: "off" */ /* eslint no-magic-numbers: ["error", { "ignore": [0, 100, 150] }] */ function $e3680afa520905a3$export$2e2bcd8739ae039(
+  array,
+  extras,
+) {
+  const map = {
+    // @ts-ignore
+    ...[].reduce.call(
+      array,
+      (map, value, index) => {
         // @ts-ignore
-        ...[].reduce.call(array, (map, value, index)=>{
-            // @ts-ignore
-            map[index] = value;
-            return map;
-        }, {}),
-        ...extras,
-        length: array.length,
-        [Symbol.iterator]: ()=>[].slice.call(map)[Symbol.iterator]()
-    };
-    return map;
+        map[index] = value;
+        return map;
+      },
+      {},
+    ),
+    ...extras,
+    length: array.length,
+    [Symbol.iterator]: () => [].slice.call(map)[Symbol.iterator](),
+  };
+  return map;
 }
 
-
-
 var $409c56140c6d134e$export$2e2bcd8739ae039 = {
-    AudioConfig: $8zHUo$microsoftcognitiveservicesspeechsdk.AudioConfig,
-    OutputFormat: $8zHUo$microsoftcognitiveservicesspeechsdk.OutputFormat,
-    ResultReason: $8zHUo$microsoftcognitiveservicesspeechsdk.ResultReason,
-    SpeechConfig: $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig,
-    SpeechRecognizer: $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechRecognizer,
-    SpeechSynthesizer: $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechSynthesizer
+  AudioConfig: $8zHUo$microsoftcognitiveservicesspeechsdk.AudioConfig,
+  OutputFormat: $8zHUo$microsoftcognitiveservicesspeechsdk.OutputFormat,
+  ResultReason: $8zHUo$microsoftcognitiveservicesspeechsdk.ResultReason,
+  SpeechConfig: $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig,
+  SpeechRecognizer: $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechRecognizer,
+  SpeechSynthesizer:
+    $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechSynthesizer,
 };
 
-
-const { ResultReason: { RecognizingSpeech: $4e800713d8cfc243$var$RecognizingSpeech, RecognizedSpeech: $4e800713d8cfc243$var$RecognizedSpeech } } = (0, $409c56140c6d134e$export$2e2bcd8739ae039);
-function $4e800713d8cfc243$export$2e2bcd8739ae039(result, { maxAlternatives: maxAlternatives = Infinity, textNormalization: textNormalization = "display" } = {}) {
-    if (result.reason === $4e800713d8cfc243$var$RecognizingSpeech || result.reason === $4e800713d8cfc243$var$RecognizedSpeech && !result.json.NBest) {
-        const resultList = [
-            {
-                confidence: 0.5,
-                transcript: result.text
-            }
-        ];
-        if (result.reason === $4e800713d8cfc243$var$RecognizedSpeech) resultList.isFinal = true;
-        return resultList;
-    } else if (result.reason === $4e800713d8cfc243$var$RecognizedSpeech) {
-        const resultList = (0, $e3680afa520905a3$export$2e2bcd8739ae039)((result.json.NBest || []).slice(0, maxAlternatives).map(({ Confidence: confidence, Display: display, ITN: itn, Lexical: lexical, MaskedITN: maskedITN })=>({
-                confidence: confidence,
-                transcript: textNormalization === "itn" ? itn : textNormalization === "lexical" ? lexical : textNormalization === "maskeditn" ? maskedITN : display
-            })), {
-            isFinal: true
-        });
-        return resultList;
-    }
-    return [];
+const {
+  ResultReason: {
+    RecognizingSpeech: $4e800713d8cfc243$var$RecognizingSpeech,
+    RecognizedSpeech: $4e800713d8cfc243$var$RecognizedSpeech,
+  },
+} = (0, $409c56140c6d134e$export$2e2bcd8739ae039);
+function $4e800713d8cfc243$export$2e2bcd8739ae039(
+  result,
+  {
+    maxAlternatives: maxAlternatives = Infinity,
+    textNormalization: textNormalization = "display",
+  } = {},
+) {
+  if (
+    result.reason === $4e800713d8cfc243$var$RecognizingSpeech ||
+    (result.reason === $4e800713d8cfc243$var$RecognizedSpeech &&
+      !result.json.NBest)
+  ) {
+    const resultList = [
+      {
+        confidence: 0.5,
+        transcript: result.text,
+      },
+    ];
+    if (result.reason === $4e800713d8cfc243$var$RecognizedSpeech)
+      resultList.isFinal = true;
+    return resultList;
+  } else if (result.reason === $4e800713d8cfc243$var$RecognizedSpeech) {
+    const resultList = (0, $e3680afa520905a3$export$2e2bcd8739ae039)(
+      (result.json.NBest || [])
+        .slice(0, maxAlternatives)
+        .map(
+          ({
+            Confidence: confidence,
+            Display: display,
+            ITN: itn,
+            Lexical: lexical,
+            MaskedITN: maskedITN,
+          }) => ({
+            isFinal: true,
+            confidence: confidence,
+            transcript:
+              textNormalization === "itn"
+                ? itn
+                : textNormalization === "lexical"
+                  ? lexical
+                  : textNormalization === "maskeditn"
+                    ? maskedITN
+                    : display,
+          }),
+        ),
+    );
+    return resultList;
+  }
+  return [];
 }
 
-
-
 function $521b485d0cd15790$export$2e2bcd8739ae039() {
-    let shiftDeferred;
-    const queue = [];
-    const push = (value)=>{
-        if (shiftDeferred) {
-            const { resolve: resolve } = shiftDeferred;
-            shiftDeferred = null;
-            resolve(value);
-        } else queue.push(value);
-    };
-    const shift = ()=>{
-        if (queue.length) return Promise.resolve(queue.shift());
-        return (shiftDeferred || (shiftDeferred = (0, ($parcel$interopDefault($8zHUo$pdeferes5)))())).promise;
-    };
-    return {
-        push: push,
-        shift: shift
-    };
+  let shiftDeferred;
+  const queue = [];
+  const push = (value) => {
+    if (shiftDeferred) {
+      const { resolve: resolve } = shiftDeferred;
+      shiftDeferred = null;
+      resolve(value);
+    } else queue.push(value);
+  };
+  const shift = () => {
+    if (queue.length) return Promise.resolve(queue.shift());
+    return (
+      shiftDeferred ||
+      (shiftDeferred = (0, $parcel$interopDefault($8zHUo$pdeferes5))())
+    ).promise;
+  };
+  return {
+    push: push,
+    shift: shift,
+  };
 }
 
-
 function $e2af95d6aca72d87$export$2e2bcd8739ae039(fnOrValue) {
-    return typeof fnOrValue === "function" ? fnOrValue() : fnOrValue;
+  return typeof fnOrValue === "function" ? fnOrValue() : fnOrValue;
 }
 
-
 let $cec424af042710dc$var$shouldWarnOnSubscriptionKey = true;
-function $cec424af042710dc$export$2e2bcd8739ae039({ credentials: credentials, looseEvents: looseEvents, ...otherOptions }) {
-    if (!credentials) throw new Error("web-speech-cognitive-services: Credentials must be specified.");
-    return {
-        ...otherOptions,
-        fetchCredentials: async ()=>{
-            const { authorizationToken: authorizationToken, customVoiceHostname: customVoiceHostname, region: region, speechRecognitionHostname: speechRecognitionHostname, speechSynthesisHostname: speechSynthesisHostname, subscriptionKey: subscriptionKey } = await (0, $e2af95d6aca72d87$export$2e2bcd8739ae039)(credentials);
-            if (!authorizationToken && !subscriptionKey || authorizationToken && subscriptionKey) throw new Error('web-speech-cognitive-services: Either "authorizationToken" or "subscriptionKey" must be provided.');
-            else if (!region && !(speechRecognitionHostname && speechSynthesisHostname)) throw new Error('web-speech-cognitive-services: Either "region" or "speechRecognitionHostname" and "speechSynthesisHostname" must be set.');
-            else if (region && (customVoiceHostname || speechRecognitionHostname || speechSynthesisHostname)) throw new Error('web-speech-cognitive-services: Only either "region" or "customVoiceHostname", "speechRecognitionHostname" and "speechSynthesisHostname" can be set.');
-            else if (authorizationToken) {
-                if (typeof authorizationToken !== "string") throw new Error('web-speech-cognitive-services: "authorizationToken" must be a string.');
-            } else if (typeof subscriptionKey !== "string") throw new Error('web-speech-cognitive-services: "subscriptionKey" must be a string.');
-            if ($cec424af042710dc$var$shouldWarnOnSubscriptionKey && subscriptionKey) {
-                console.warn("web-speech-cognitive-services: In production environment, subscription key should not be used, authorization token should be used instead.");
-                $cec424af042710dc$var$shouldWarnOnSubscriptionKey = false;
-            }
-            const resolvedCredentials = {
-                region: region,
-                authorizationToken: authorizationToken,
-                subscriptionKey: subscriptionKey,
-                customVoiceHostname: customVoiceHostname,
-                speechRecognitionHostname: speechRecognitionHostname,
-                speechSynthesisHostname: speechSynthesisHostname
-            };
-            return resolvedCredentials;
-        },
-        looseEvents: looseEvents
-    };
+function $cec424af042710dc$export$2e2bcd8739ae039({
+  credentials: credentials,
+  looseEvents: looseEvents,
+  ...otherOptions
+}) {
+  if (!credentials)
+    throw new Error(
+      "web-speech-cognitive-services: Credentials must be specified.",
+    );
+  return {
+    ...otherOptions,
+    fetchCredentials: async () => {
+      const {
+        authorizationToken: authorizationToken,
+        customVoiceHostname: customVoiceHostname,
+        region: region,
+        speechRecognitionHostname: speechRecognitionHostname,
+        speechSynthesisHostname: speechSynthesisHostname,
+        subscriptionKey: subscriptionKey,
+      } = await (0, $e2af95d6aca72d87$export$2e2bcd8739ae039)(credentials);
+      if (
+        (!authorizationToken && !subscriptionKey) ||
+        (authorizationToken && subscriptionKey)
+      )
+        throw new Error(
+          'web-speech-cognitive-services: Either "authorizationToken" or "subscriptionKey" must be provided.',
+        );
+      else if (
+        !region &&
+        !(speechRecognitionHostname && speechSynthesisHostname)
+      )
+        throw new Error(
+          'web-speech-cognitive-services: Either "region" or "speechRecognitionHostname" and "speechSynthesisHostname" must be set.',
+        );
+      else if (
+        region &&
+        (customVoiceHostname ||
+          speechRecognitionHostname ||
+          speechSynthesisHostname)
+      )
+        throw new Error(
+          'web-speech-cognitive-services: Only either "region" or "customVoiceHostname", "speechRecognitionHostname" and "speechSynthesisHostname" can be set.',
+        );
+      else if (authorizationToken) {
+        if (typeof authorizationToken !== "string")
+          throw new Error(
+            'web-speech-cognitive-services: "authorizationToken" must be a string.',
+          );
+      } else if (typeof subscriptionKey !== "string")
+        throw new Error(
+          'web-speech-cognitive-services: "subscriptionKey" must be a string.',
+        );
+      if (
+        $cec424af042710dc$var$shouldWarnOnSubscriptionKey &&
+        subscriptionKey
+      ) {
+        console.warn(
+          "web-speech-cognitive-services: In production environment, subscription key should not be used, authorization token should be used instead.",
+        );
+        $cec424af042710dc$var$shouldWarnOnSubscriptionKey = false;
+      }
+      const resolvedCredentials = {
+        region: region,
+        authorizationToken: authorizationToken,
+        subscriptionKey: subscriptionKey,
+        customVoiceHostname: customVoiceHostname,
+        speechRecognitionHostname: speechRecognitionHostname,
+        speechSynthesisHostname: speechSynthesisHostname,
+      };
+      return resolvedCredentials;
+    },
+    looseEvents: looseEvents,
+  };
 }
 
-
 /* eslint class-methods-use-this: "off" */ class $c213b3979e1323e8$export$2e2bcd8739ae039 {
-    addFromString() {
-        throw new Error("JSGF is not supported");
-    }
-    get phrases() {
-        return this._phrases;
-    }
-    set phrases(value) {
-        if (Array.isArray(value)) this._phrases = value;
-        else if (typeof value === "string") this._phrases = [
-            value
-        ];
-        else throw new Error(`The provided value is not an array or of type 'string'`);
-    }
-    constructor(){
-        this._phrases = [];
-    }
+  addFromString() {
+    throw new Error("JSGF is not supported");
+  }
+  get phrases() {
+    return this._phrases;
+  }
+  set phrases(value) {
+    if (Array.isArray(value)) this._phrases = value;
+    else if (typeof value === "string") this._phrases = [value];
+    else
+      throw new Error(`The provided value is not an array or of type 'string'`);
+  }
+  constructor() {
+    this._phrases = [];
+  }
 }
 
-
-
 // https://docs.microsoft.com/en-us/javascript/api/microsoft-cognitiveservices-speech-sdk/speechconfig?view=azure-node-latest#outputformat
 // {
 //   "RecognitionStatus": "Success",
@@ -175,956 +286,1301 @@ function $cec424af042710dc$export$2e2bcd8739ae039({ credentials: credentials, lo
 //   "Duration": 0
 // }
 // const { AudioConfig, OutputFormat, ResultReason, SpeechConfig, SpeechRecognizer } = SDK;
-function $87729c1c3277125d$var$serializeRecognitionResult({ duration: duration, errorDetails: errorDetails, json: json, offset: offset, properties: properties, reason: reason, resultId: resultId, text: text }) {
-    return {
-        duration: duration,
-        errorDetails: errorDetails,
-        json: JSON.parse(json),
-        offset: offset,
-        properties: properties,
-        reason: reason,
-        resultId: resultId,
-        text: text
-    };
+function $87729c1c3277125d$var$serializeRecognitionResult({
+  duration: duration,
+  errorDetails: errorDetails,
+  json: json,
+  offset: offset,
+  properties: properties,
+  reason: reason,
+  resultId: resultId,
+  text: text,
+}) {
+  return {
+    duration: duration,
+    errorDetails: errorDetails,
+    json: JSON.parse(json),
+    offset: offset,
+    properties: properties,
+    reason: reason,
+    resultId: resultId,
+    text: text,
+  };
 }
 function $87729c1c3277125d$var$averageAmplitude(arrayBuffer) {
-    const array = new Int16Array(arrayBuffer);
-    return array.reduce((averageAmplitude, amplitude)=>averageAmplitude + Math.abs(amplitude), 0) / array.length;
+  const array = new Int16Array(arrayBuffer);
+  return (
+    array.reduce(
+      (averageAmplitude, amplitude) => averageAmplitude + Math.abs(amplitude),
+      0,
+    ) / array.length
+  );
 }
 function $87729c1c3277125d$var$cognitiveServicesAsyncToPromise(fn) {
-    return (...args)=>new Promise((resolve, reject)=>fn(...args, resolve, reject));
+  return (...args) =>
+    new Promise((resolve, reject) => fn(...args, resolve, reject));
 }
 function $87729c1c3277125d$var$prepareAudioConfig(audioConfig) {
-    const originalAttach = audioConfig.attach;
-    const boundOriginalAttach = audioConfig.attach.bind(audioConfig);
-    let firstChunk = false;
-    let muted = false;
-    // We modify "attach" function and detect when audible chunk is read.
-    // We will only modify "attach" function once.
-    audioConfig.attach = async ()=>{
-        const reader = await boundOriginalAttach();
-        return {
-            ...reader,
-            read: async ()=>{
-                const chunk = await reader.read();
-                // The magic number 150 is measured by:
-                // 1. Set microphone volume to 0
-                // 2. Observe the amplitude (100-110) for the first few chunks
-                //    (There is a short static caught when turning on the microphone)
-                // 3. Set the number a bit higher than the observation
-                if (!firstChunk && $87729c1c3277125d$var$averageAmplitude(chunk.buffer) > 150) {
-                    audioConfig.events.onEvent({
-                        name: "FirstAudibleChunk"
-                    });
-                    firstChunk = true;
-                }
-                if (muted) return {
-                    buffer: new ArrayBuffer(0),
-                    isEnd: true,
-                    timeReceived: Date.now()
-                };
-                return chunk;
-            }
-        };
-    };
+  const originalAttach = audioConfig.attach;
+  const boundOriginalAttach = audioConfig.attach.bind(audioConfig);
+  let firstChunk = false;
+  let muted = false;
+  // We modify "attach" function and detect when audible chunk is read.
+  // We will only modify "attach" function once.
+  audioConfig.attach = async () => {
+    const reader = await boundOriginalAttach();
     return {
-        audioConfig: audioConfig,
-        pause: ()=>{
-            muted = true;
-        },
-        unprepare: ()=>{
-            audioConfig.attach = originalAttach;
+      ...reader,
+      read: async () => {
+        const chunk = await reader.read();
+        // The magic number 150 is measured by:
+        // 1. Set microphone volume to 0
+        // 2. Observe the amplitude (100-110) for the first few chunks
+        //    (There is a short static caught when turning on the microphone)
+        // 3. Set the number a bit higher than the observation
+        if (
+          !firstChunk &&
+          $87729c1c3277125d$var$averageAmplitude(chunk.buffer) > 150
+        ) {
+          audioConfig.events.onEvent({
+            name: "FirstAudibleChunk",
+          });
+          firstChunk = true;
         }
+        if (muted)
+          return {
+            buffer: new ArrayBuffer(0),
+            isEnd: true,
+            timeReceived: Date.now(),
+          };
+        return chunk;
+      },
     };
+  };
+  return {
+    audioConfig: audioConfig,
+    pause: () => {
+      muted = true;
+    },
+    unprepare: () => {
+      audioConfig.attach = originalAttach;
+    },
+  };
 }
 class $87729c1c3277125d$export$7a5e8c1807e6b8ee {
-    constructor(options, data){
-        this.audioConfig = null;
-        this.speechConfig = null;
-        this.recognizer = null;
-        this.enableTelemetry = true;
-        this.looseEvents = false;
-        this.textNormalization = "display";
-        this.started = false;
-        this._autoStart = false;
-        this._passive = false;
+  constructor(options, data) {
+    this.audioConfig = null;
+    this.speechConfig = null;
+    this.recognizer = null;
+    this.enableTelemetry = true;
+    this.looseEvents = false;
+    this.textNormalization = "display";
+    this.started = false;
+    this._autoStart = false;
+    this._passive = false;
+    this._continuous = false;
+    this._interimResults = true;
+    this._grammars = new (0, $c213b3979e1323e8$export$2e2bcd8739ae039)();
+    this._maxAlternatives = 1;
+    this._debug = false;
+    this.onstart = () => {};
+    this.onend = () => {};
+    this.onpassivestart = () => {};
+    this.onpassiveend = () => {};
+    this.onaudiostart = () => {};
+    this.onaudioend = () => {};
+    this.onsoundstart = () => {};
+    this.onsoundend = () => {};
+    this.onspeechstart = () => {};
+    this.onspeechend = () => {};
+    this.onerror = (value) => {
+      console.log("Error : ", value);
+    };
+    this.onabort = () => {
+      this._debug && console.log("Recognition aborted");
+    };
+    this.onresult = (value) => {
+      this._debug && console.log("Result : ", value);
+    };
+    this.onpassiveresult = (value) => {
+      this._debug && console.log("Passive Result : ", value);
+    };
+    this.onwakeup = () => {
+      this._debug && console.log("Wake up !");
+    };
+    this.start = () => {
+      this._startOnce().catch((err) => {
+        new Error(`error : ${err.message}\ncallstack : ${err.stack}`);
+      });
+    };
+    this.abort = undefined;
+    this.stop = undefined;
+    /**
+     * Retrieval of credentials, initialization of speechConfig and start recognizing
+     * @param fetchCredentials Function
+     * @param speechRecognitionEndpointId string | undefined
+     */ this.initRecognizer = async (
+      fetchCredentials,
+      speechRecognitionEndpointId,
+      timerBeforeSpeechEnd,
+    ) => {
+      const {
+        authorizationToken: authorizationToken,
+        region: region = "westus",
+        speechRecognitionHostname: speechRecognitionHostname,
+        subscriptionKey: subscriptionKey,
+      } = await fetchCredentials();
+      if (speechRecognitionHostname) {
+        if (authorizationToken) {
+          this.speechConfig =
+            $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromHost(
+              new URL(`wss://${speechRecognitionHostname}`),
+            );
+          this.speechConfig.authorizationToken = authorizationToken;
+        } else
+          this.speechConfig =
+            $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromHost(
+              new URL(`wss://${speechRecognitionHostname}`),
+              subscriptionKey,
+            );
+      } else if (region && (authorizationToken || subscriptionKey))
+        this.speechConfig = authorizationToken
+          ? $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromAuthorizationToken(
+              authorizationToken,
+              region,
+            )
+          : subscriptionKey
+            ? $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromSubscription(
+                subscriptionKey,
+                region,
+              )
+            : null;
+      if (this.speechConfig) {
+        if (speechRecognitionEndpointId && this.speechConfig)
+          this.speechConfig.endpointId = speechRecognitionEndpointId;
+        this.speechConfig.outputFormat =
+          $8zHUo$microsoftcognitiveservicesspeechsdk.OutputFormat.Detailed;
+        this.speechConfig.speechRecognitionLanguage = this._lang || "en-US";
+        timerBeforeSpeechEnd &&
+          this.speechConfig.setProperty(
+            $8zHUo$microsoftcognitiveservicesspeechsdk.PropertyId[
+              $8zHUo$microsoftcognitiveservicesspeechsdk.PropertyId
+                .Speech_SegmentationSilenceTimeoutMs
+            ],
+            `${timerBeforeSpeechEnd}`,
+          );
+        this._autoStart && this.start();
+      }
+    };
+    /**
+     * Create a new Synthesizer from audioConfig / speechConfig / lang
+     * @param lang string
+     */ this.createRecognizer = async () => {
+      if (this.audioConfig && this.speechConfig) {
+        this.speechConfig.speechRecognitionLanguage = this._lang;
+        this.recognizer =
+          new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechRecognizer(
+            this.speechConfig,
+            this.audioConfig,
+          );
+        // Add grammars
+        const { dynamicGrammar: dynamicGrammar } = this.recognizer.internalData;
+        this.referenceGrammars &&
+          this.referenceGrammars.length &&
+          dynamicGrammar.addReferenceGrammar(this.referenceGrammars);
+        // Add phrases
+        const { phrases: phrases } = this._grammars;
+        if (phrases && phrases.length) {
+          const phraseList =
+            $8zHUo$microsoftcognitiveservicesspeechsdk.PhraseListGrammar.fromRecognizer(
+              this.recognizer,
+            );
+          phrases.forEach((phrase) => {
+            phraseList.addPhrase(phrase);
+          });
+        }
+      } else this.recognizer = null;
+    };
+    /**
+     * Stop current recognizer, change language and start a new recognition
+     * @param lang string
+     */ this.changeLanguage = async (lang) => {
+      if (
+        this.recognizer &&
+        this.audioConfig &&
+        this.speechConfig &&
+        lang &&
+        lang !== this._lang
+      ) {
+        // Stop current recognition and start a new one
+        await $87729c1c3277125d$var$cognitiveServicesAsyncToPromise(
+          this.recognizer.stopContinuousRecognitionAsync.bind(this.recognizer),
+        )();
+        this._lang = lang;
+        this._continuous && this.start();
+      } else this.recognizer = null;
+    };
+    /**
+     * In continuous mode, toggle from passive to active mode by stopping current recognition and starting a new one to prevent
+     * receiving results from a current passive speech recognition.
+     * If you don't care about having existing results in active recognition, just set recognition's 'passive' variable to 'false' instead
+     * of using this method.
+     */ this.toggleContinuousPassiveToActive = async () => {
+      if (
+        this._continuous &&
+        this.recognizer &&
+        this.audioConfig &&
+        this.speechConfig
+      ) {
+        // Stop current recognition and start a new one
+        if (this.started)
+          await $87729c1c3277125d$var$cognitiveServicesAsyncToPromise(
+            this.recognizer.stopContinuousRecognitionAsync.bind(
+              this.recognizer,
+            ),
+          )();
         this._continuous = false;
-        this._interimResults = true;
-        this._grammars = new (0, $c213b3979e1323e8$export$2e2bcd8739ae039)();
-        this._maxAlternatives = 1;
-        this._debug = false;
-        this.onstart = ()=>{};
-        this.onend = ()=>{};
-        this.onpassivestart = ()=>{};
-        this.onpassiveend = ()=>{};
-        this.onaudiostart = ()=>{};
-        this.onaudioend = ()=>{};
-        this.onsoundstart = ()=>{};
-        this.onsoundend = ()=>{};
-        this.onspeechstart = ()=>{};
-        this.onspeechend = ()=>{};
-        this.onerror = (value)=>{
-            console.log("Error : ", value);
-        };
-        this.onabort = ()=>{
-            this._debug && console.log("Recognition aborted");
-        };
-        this.onresult = (value)=>{
-            this._debug && console.log("Result : ", value);
-        };
-        this.onpassiveresult = (value)=>{
-            this._debug && console.log("Passive Result : ", value);
-        };
-        this.onwakeup = ()=>{
-            this._debug && console.log("Wake up !");
-        };
-        this.start = ()=>{
-            this._startOnce().catch((err)=>{
-                new Error(`error : ${err.message}\ncallstack : ${err.stack}`);
+        this._passive = false;
+        this.start();
+      }
+    };
+    /**
+     * In continuous mode, toggle from passive to active mode by stopping current recognition and starting a new one to prevent
+     * receiving results from a current passive speech recognition.
+     * If you don't care about having existing results in active recognition, just set recognition's 'passive' variable to 'false' instead
+     * of using this method.
+     */ this.toggleContinuousActiveToPassive = async () => {
+      if (
+        !this._continuous &&
+        this.recognizer &&
+        this.audioConfig &&
+        this.speechConfig
+      ) {
+        // Stop current recognition and start a new one
+        if (this.started)
+          await $87729c1c3277125d$var$cognitiveServicesAsyncToPromise(
+            this.recognizer.stopContinuousRecognitionAsync.bind(
+              this.recognizer,
+            ),
+          )();
+        this._continuous = true;
+        this._passive = true;
+        this.start();
+      }
+    };
+    this.processSendEvent = (type, data) => {
+      this._debug &&
+        console.log(
+          "[[Speech Recognizer Event]] : type = ",
+          type,
+          "\n, data = ",
+          data,
+        );
+      switch (type) {
+        case "start":
+          if (this._passive) this.onpassivestart && this.onpassivestart();
+          else this.onstart && this.onstart();
+          this.started = true;
+          break;
+        case "end":
+          if (this._passive) this.onpassiveend && this.onpassiveend();
+          else this.onend && this.onend();
+          this.started = false;
+          break;
+        case "audiostart":
+          this.onaudiostart && this.onaudiostart();
+          break;
+        case "audioend":
+          this.onaudioend && this.onaudioend();
+          break;
+        case "soundstart":
+          this.onsoundstart && this.onsoundstart();
+          break;
+        case "soundend":
+          this.onsoundend && this.onsoundend();
+          break;
+        case "speechstart":
+          this.onspeechstart && this.onspeechstart();
+          break;
+        case "speechend":
+          this.onspeechend && this.onspeechend();
+          break;
+        case "error":
+          this.onerror && this.onerror(data);
+          this.started = false;
+          break;
+        case "abort":
+          this.onabort && this.onabort();
+          this.started = false;
+          break;
+        case "result":
+          this.onresult && this.onresult(data.results);
+          break;
+        case "passiveresult":
+          this.onpassiveresult && this.onpassiveresult(data.results);
+          break;
+        case "wakeup":
+          this.onwakeup && this.onwakeup();
+          break;
+      }
+    };
+    this._autoStart = !!data?.autoStart;
+    this._passive = !!data?.passive;
+    this._wakeWords = data?.wakeWords || [];
+    this._continuous = !!data?.continuous;
+    this._interimResults = !(data?.interimResults === false);
+    this._lang = data?.lang
+      ? data.lang
+      : typeof window !== "undefined"
+        ? window.document.documentElement.getAttribute("lang") ||
+          window.navigator.language
+        : "en-US";
+    data?.grammarsList && (this._grammars.phrases = data.grammarsList);
+    this._debug = !!data?.debug || false;
+    const {
+      audioConfig: audioConfig = null,
+      // We set telemetry to true to honor the default telemetry settings of Speech SDK
+      enableTelemetry:
+        // https://github.com/Microsoft/cognitive-services-speech-sdk-js#data--telemetry
+        enableTelemetry = true,
+      fetchCredentials: fetchCredentials,
+      looseEvents: looseEvents,
+      referenceGrammars: referenceGrammars,
+      speechRecognitionEndpointId: speechRecognitionEndpointId,
+      textNormalization: textNormalization = "display",
+    } = (0, $cec424af042710dc$export$2e2bcd8739ae039)(options);
+    this.enableTelemetry = enableTelemetry;
+    this.looseEvents = !!looseEvents;
+    this.referenceGrammars = referenceGrammars;
+    this.textNormalization = textNormalization;
+    if (
+      !audioConfig &&
+      (!window.navigator.mediaDevices ||
+        !window.navigator.mediaDevices.getUserMedia)
+    )
+      console.warn(
+        "web-speech-cognitive-services: This browser does not support WebRTC and it will not work with Cognitive Services Speech Services.",
+      );
+    else
+      try {
+        this.audioConfig =
+          audioConfig ||
+          $8zHUo$microsoftcognitiveservicesspeechsdk.AudioConfig.fromDefaultMicrophoneInput();
+        this.initRecognizer(
+          fetchCredentials,
+          speechRecognitionEndpointId,
+          data?.timerBeforeSpeechEnd,
+        );
+      } catch (e) {
+        console.warn(e);
+      }
+  }
+  get passive() {
+    return this._passive;
+  }
+  set passive(value) {
+    this._passive = value;
+  }
+  get wakeWords() {
+    return this._wakeWords;
+  }
+  set wakeWords(value) {
+    this._wakeWords = value;
+  }
+  get continuous() {
+    return this._continuous;
+  }
+  set continuous(value) {
+    this._continuous = value;
+  }
+  get grammars() {
+    return this._grammars;
+  }
+  set grammars(value) {
+    if (value instanceof (0, $c213b3979e1323e8$export$2e2bcd8739ae039))
+      this._grammars = value;
+    else
+      throw new Error(`The provided value is not of type 'SpeechGrammarList'`);
+  }
+  get interimResults() {
+    return this._interimResults;
+  }
+  set interimResults(value) {
+    this._interimResults = value;
+  }
+  get maxAlternatives() {
+    return this._maxAlternatives;
+  }
+  set maxAlternatives(value) {
+    this._maxAlternatives = value;
+  }
+  get lang() {
+    return this._lang;
+  }
+  set lang(value) {
+    this._lang = value;
+  }
+  async _startOnce() {
+    if (this.audioConfig && !this.started) {
+      const { pause: pause, unprepare: unprepare } =
+        $87729c1c3277125d$var$prepareAudioConfig(this.audioConfig);
+      const queue = (0, $521b485d0cd15790$export$2e2bcd8739ae039)();
+      let soundStarted = false;
+      let speechStarted = false;
+      let stopping = "";
+      const { detach: detachAudioConfigEvent } = this.audioConfig.events.attach(
+        (event) => {
+          const { name: name } = event;
+          if (name === "AudioSourceReadyEvent")
+            queue.push({
+              audioSourceReady: {},
             });
-        };
-        this.abort = undefined;
-        this.stop = undefined;
-        /**
-   * Retrieval of credentials, initialization of speechConfig and start recognizing
-   * @param fetchCredentials Function
-   * @param speechRecognitionEndpointId string | undefined
-   */ this.initRecognizer = async (fetchCredentials, speechRecognitionEndpointId, timerBeforeSpeechEnd)=>{
-            const { authorizationToken: authorizationToken, region: region = "westus", speechRecognitionHostname: speechRecognitionHostname, subscriptionKey: subscriptionKey } = await fetchCredentials();
-            if (speechRecognitionHostname) {
-                if (authorizationToken) {
-                    this.speechConfig = $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromHost(new URL(`wss://${speechRecognitionHostname}`));
-                    this.speechConfig.authorizationToken = authorizationToken;
-                } else this.speechConfig = $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromHost(new URL(`wss://${speechRecognitionHostname}`), subscriptionKey);
-            } else if (region && (authorizationToken || subscriptionKey)) this.speechConfig = authorizationToken ? $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromAuthorizationToken(authorizationToken, region) : subscriptionKey ? $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromSubscription(subscriptionKey, region) : null;
-            if (this.speechConfig) {
-                if (speechRecognitionEndpointId && this.speechConfig) this.speechConfig.endpointId = speechRecognitionEndpointId;
-                this.speechConfig.outputFormat = $8zHUo$microsoftcognitiveservicesspeechsdk.OutputFormat.Detailed;
-                this.speechConfig.speechRecognitionLanguage = this._lang || "en-US";
-                timerBeforeSpeechEnd && this.speechConfig.setProperty($8zHUo$microsoftcognitiveservicesspeechsdk.PropertyId[$8zHUo$microsoftcognitiveservicesspeechsdk.PropertyId.Speech_SegmentationSilenceTimeoutMs], `${timerBeforeSpeechEnd}`);
-                this._autoStart && this.start();
+          else if (name === "AudioSourceOffEvent")
+            queue.push({
+              audioSourceOff: {},
+            });
+          else if (name === "FirstAudibleChunk")
+            queue.push({
+              firstAudibleChunk: {},
+            });
+        },
+      );
+      await this.createRecognizer();
+      if (this.recognizer)
+        try {
+          this.recognizer.canceled = (
+            _,
+            {
+              errorDetails: errorDetails,
+              offset: offset,
+              reason: reason,
+              sessionId: sessionId,
+            },
+          ) => {
+            queue.push({
+              canceled: {
+                errorDetails: errorDetails,
+                offset: offset,
+                reason: reason,
+                sessionId: sessionId,
+              },
+            });
+          };
+          this.recognizer.recognized = (
+            _,
+            { offset: offset, result: result, sessionId: sessionId },
+          ) => {
+            queue.push({
+              recognized: {
+                offset: offset,
+                result:
+                  $87729c1c3277125d$var$serializeRecognitionResult(result),
+                sessionId: sessionId,
+              },
+            });
+          };
+          this.recognizer.recognizing = (
+            _,
+            { offset: offset, result: result, sessionId: sessionId },
+          ) => {
+            queue.push({
+              recognizing: {
+                offset: offset,
+                result:
+                  $87729c1c3277125d$var$serializeRecognitionResult(result),
+                sessionId: sessionId,
+              },
+            });
+          };
+          this.recognizer.sessionStarted = (_, { sessionId: sessionId }) => {
+            queue.push({
+              sessionStarted: {
+                sessionId: sessionId,
+              },
+            });
+          };
+          this.recognizer.sessionStopped = (_, { sessionId: sessionId }) => {
+            // "sessionStopped" is never fired, probably because we are using startContinuousRecognitionAsync instead of recognizeOnceAsync.
+            queue.push({
+              sessionStopped: {
+                sessionId: sessionId,
+              },
+            });
+          };
+          this.recognizer.speechStartDetected = (
+            _,
+            { offset: offset, sessionId: sessionId },
+          ) => {
+            queue.push({
+              speechStartDetected: {
+                offset: offset,
+                sessionId: sessionId,
+              },
+            });
+          };
+          this.recognizer.speechEndDetected = (_, { sessionId: sessionId }) => {
+            // "speechEndDetected" is never fired, probably because we are using startContinuousRecognitionAsync instead of recognizeOnceAsync.
+            // Update: "speechEndDetected" is fired for DLSpeech.listenOnceAsync()
+            queue.push({
+              speechEndDetected: {
+                sessionId: sessionId,
+              },
+            });
+          };
+          await $87729c1c3277125d$var$cognitiveServicesAsyncToPromise(
+            this.recognizer.startContinuousRecognitionAsync.bind(
+              this.recognizer,
+            ),
+          )();
+          this.abort = () =>
+            queue.push({
+              abort: {},
+            });
+          this.stop = () =>
+            queue.push({
+              stop: {},
+            });
+          let audioStarted = false;
+          let finalEvent = null;
+          let finalizedResults = [];
+          for (let loop = 0; !stopping || audioStarted; loop++) {
+            const event = await queue.shift();
+            const {
+              abort: abort,
+              audioSourceOff: audioSourceOff,
+              audioSourceReady: audioSourceReady,
+              canceled: canceled,
+              firstAudibleChunk: firstAudibleChunk,
+              recognized: recognized,
+              recognizing: recognizing,
+              stop: stop,
+            } = event;
+            const errorMessage = canceled && canceled.errorDetails;
+            if (/Permission\sdenied/u.test(errorMessage || "")) {
+              // If microphone is not allowed, we should not emit "start" event.
+              finalEvent = {
+                data: "not-allowed",
+                type: "error",
+              };
+              break;
             }
-        };
-        /**
-   * Create a new Synthesizer from audioConfig / speechConfig / lang
-   * @param lang string
-   */ this.createRecognizer = async ()=>{
-            if (this.audioConfig && this.speechConfig) {
-                this.speechConfig.speechRecognitionLanguage = this._lang;
-                this.recognizer = new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechRecognizer(this.speechConfig, this.audioConfig);
-                // Add grammars
-                const { dynamicGrammar: dynamicGrammar } = this.recognizer.internalData;
-                this.referenceGrammars && this.referenceGrammars.length && dynamicGrammar.addReferenceGrammar(this.referenceGrammars);
-                // Add phrases
-                const { phrases: phrases } = this._grammars;
-                if (phrases && phrases.length) {
-                    const phraseList = $8zHUo$microsoftcognitiveservicesspeechsdk.PhraseListGrammar.fromRecognizer(this.recognizer);
-                    phrases.forEach((phrase)=>{
-                        phraseList.addPhrase(phrase);
-                    });
+            if (!loop) this.processSendEvent("start");
+            if (errorMessage) {
+              if (/1006/u.test(errorMessage)) {
+                if (!audioStarted) {
+                  this.processSendEvent("audiostart");
+                  this.processSendEvent("audioend");
                 }
-            } else this.recognizer = null;
-        };
-        /**
-   * Stop current recognizer, change language and start a new recognition
-   * @param lang string
-   */ this.changeLanguage = async (lang)=>{
-            if (this.recognizer && this.audioConfig && this.speechConfig && lang && lang !== this._lang) {
-                // Stop current recognition and start a new one
-                await $87729c1c3277125d$var$cognitiveServicesAsyncToPromise(this.recognizer.stopContinuousRecognitionAsync.bind(this.recognizer))();
-                this._lang = lang;
-                this._continuous && this.start();
-            } else this.recognizer = null;
-        };
-        /**
-   * In continuous mode, toggle from passive to active mode by stopping current recognition and starting a new one to prevent
-   * receiving results from a current passive speech recognition.
-   * If you don't care about having existing results in active recognition, just set recognition's 'passive' variable to 'false' instead
-   * of using this method.
-   */ this.toggleContinuousPassiveToActive = async ()=>{
-            if (this._continuous && this.recognizer && this.audioConfig && this.speechConfig) {
-                // Stop current recognition and start a new one
-                if (this.started) await $87729c1c3277125d$var$cognitiveServicesAsyncToPromise(this.recognizer.stopContinuousRecognitionAsync.bind(this.recognizer))();
-                this._continuous = false;
-                this._passive = false;
-                this.start();
-            }
-        };
-        /**
-   * In continuous mode, toggle from passive to active mode by stopping current recognition and starting a new one to prevent
-   * receiving results from a current passive speech recognition.
-   * If you don't care about having existing results in active recognition, just set recognition's 'passive' variable to 'false' instead
-   * of using this method.
-   */ this.toggleContinuousActiveToPassive = async ()=>{
-            if (!this._continuous && this.recognizer && this.audioConfig && this.speechConfig) {
-                // Stop current recognition and start a new one
-                if (this.started) await $87729c1c3277125d$var$cognitiveServicesAsyncToPromise(this.recognizer.stopContinuousRecognitionAsync.bind(this.recognizer))();
-                this._continuous = true;
-                this._passive = true;
-                this.start();
-            }
-        };
-        this.processSendEvent = (type, data)=>{
-            this._debug && console.log("Speech Recognizer Event : type = ", type, "\n, data = ", data);
-            switch(type){
-                case "start":
-                    if (this._passive) this.onpassivestart && this.onpassivestart();
-                    else this.onstart && this.onstart();
-                    this.started = true;
-                    break;
-                case "end":
-                    if (this._passive) this.onpassiveend && this.onpassiveend();
-                    else this.onend && this.onend();
-                    this.started = false;
-                    break;
-                case "audiostart":
-                    this.onaudiostart && this.onaudiostart();
-                    break;
-                case "audioend":
-                    this.onaudioend && this.onaudioend();
-                    break;
-                case "soundstart":
-                    this.onsoundstart && this.onsoundstart();
-                    break;
-                case "soundend":
-                    this.onsoundend && this.onsoundend();
-                    break;
-                case "speechstart":
-                    this.onspeechstart && this.onspeechstart();
-                    break;
-                case "speechend":
-                    this.onspeechend && this.onspeechend();
-                    break;
-                case "error":
-                    this.onerror && this.onerror(data);
-                    this.started = false;
-                    break;
-                case "abort":
-                    this.onabort && this.onabort();
-                    this.started = false;
-                    break;
-                case "result":
-                    this.onresult && this.onresult(data.results);
-                    break;
-                case "passiveresult":
-                    this.onpassiveresult && this.onpassiveresult(data.results);
-                    break;
-                case "wakeup":
-                    this.onwakeup && this.onwakeup();
-                    break;
-            }
-        };
-        this._autoStart = !!data?.autoStart;
-        this._passive = !!data?.passive;
-        this._wakeWords = data?.wakeWords || [];
-        this._continuous = !!data?.continuous;
-        this._interimResults = !(data?.interimResults === false);
-        this._lang = data?.lang ? data.lang : typeof window !== "undefined" ? window.document.documentElement.getAttribute("lang") || window.navigator.language : "en-US";
-        data?.grammarsList && (this._grammars.phrases = data.grammarsList);
-        this._debug = !!data?.debug || false;
-        const { audioConfig: audioConfig = null, enableTelemetry: // We set telemetry to true to honor the default telemetry settings of Speech SDK
-        // https://github.com/Microsoft/cognitive-services-speech-sdk-js#data--telemetry
-        enableTelemetry = true, fetchCredentials: fetchCredentials, looseEvents: looseEvents, referenceGrammars: referenceGrammars, speechRecognitionEndpointId: speechRecognitionEndpointId, textNormalization: textNormalization = "display" } = (0, $cec424af042710dc$export$2e2bcd8739ae039)(options);
-        this.enableTelemetry = enableTelemetry;
-        this.looseEvents = !!looseEvents;
-        this.referenceGrammars = referenceGrammars;
-        this.textNormalization = textNormalization;
-        if (!audioConfig && (!window.navigator.mediaDevices || !window.navigator.mediaDevices.getUserMedia)) console.warn("web-speech-cognitive-services: This browser does not support WebRTC and it will not work with Cognitive Services Speech Services.");
-        else try {
-            this.audioConfig = audioConfig || $8zHUo$microsoftcognitiveservicesspeechsdk.AudioConfig.fromDefaultMicrophoneInput();
-            this.initRecognizer(fetchCredentials, speechRecognitionEndpointId, data?.timerBeforeSpeechEnd);
-        } catch (e) {
-            console.warn(e);
-        }
-    }
-    get passive() {
-        return this._passive;
-    }
-    set passive(value) {
-        this._passive = value;
-    }
-    get wakeWords() {
-        return this._wakeWords;
-    }
-    set wakeWords(value) {
-        this._wakeWords = value;
-    }
-    get continuous() {
-        return this._continuous;
-    }
-    set continuous(value) {
-        this._continuous = value;
-    }
-    get grammars() {
-        return this._grammars;
-    }
-    set grammars(value) {
-        if (value instanceof (0, $c213b3979e1323e8$export$2e2bcd8739ae039)) this._grammars = value;
-        else throw new Error(`The provided value is not of type 'SpeechGrammarList'`);
-    }
-    get interimResults() {
-        return this._interimResults;
-    }
-    set interimResults(value) {
-        this._interimResults = value;
-    }
-    get maxAlternatives() {
-        return this._maxAlternatives;
-    }
-    set maxAlternatives(value) {
-        this._maxAlternatives = value;
-    }
-    get lang() {
-        return this._lang;
-    }
-    set lang(value) {
-        this._lang = value;
-    }
-    async _startOnce() {
-        if (this.audioConfig && !this.started) {
-            const { pause: pause, unprepare: unprepare } = $87729c1c3277125d$var$prepareAudioConfig(this.audioConfig);
-            const queue = (0, $521b485d0cd15790$export$2e2bcd8739ae039)();
-            let soundStarted = false;
-            let speechStarted = false;
-            let stopping = "";
-            const { detach: detachAudioConfigEvent } = this.audioConfig.events.attach((event)=>{
-                const { name: name } = event;
-                if (name === "AudioSourceReadyEvent") queue.push({
-                    audioSourceReady: {}
-                });
-                else if (name === "AudioSourceOffEvent") queue.push({
-                    audioSourceOff: {}
-                });
-                else if (name === "FirstAudibleChunk") queue.push({
-                    firstAudibleChunk: {}
-                });
-            });
-            await this.createRecognizer();
-            if (this.recognizer) try {
-                this.recognizer.canceled = (_, { errorDetails: errorDetails, offset: offset, reason: reason, sessionId: sessionId })=>{
-                    queue.push({
-                        canceled: {
-                            errorDetails: errorDetails,
-                            offset: offset,
-                            reason: reason,
-                            sessionId: sessionId
-                        }
-                    });
+                finalEvent = {
+                  data: "network",
+                  type: "error",
                 };
-                this.recognizer.recognized = (_, { offset: offset, result: result, sessionId: sessionId })=>{
-                    queue.push({
-                        recognized: {
-                            offset: offset,
-                            result: $87729c1c3277125d$var$serializeRecognitionResult(result),
-                            sessionId: sessionId
-                        }
-                    });
+              } else
+                finalEvent = {
+                  data: "unknown",
+                  type: "error",
                 };
-                this.recognizer.recognizing = (_, { offset: offset, result: result, sessionId: sessionId })=>{
-                    queue.push({
-                        recognizing: {
-                            offset: offset,
-                            result: $87729c1c3277125d$var$serializeRecognitionResult(result),
-                            sessionId: sessionId
-                        }
-                    });
+              break;
+            } else if (abort || stop) {
+              if (abort) {
+                finalEvent = {
+                  type: "abort",
                 };
-                this.recognizer.sessionStarted = (_, { sessionId: sessionId })=>{
-                    queue.push({
-                        sessionStarted: {
-                            sessionId: sessionId
-                        }
-                    });
+                // If we are aborting, we will ignore lingering recognizing/recognized events. But if we are stopping, we need them.
+                stopping = "abort";
+              } else {
+                // When we pause, we will send { isEnd: true }, Speech Services will send us "recognized" event.
+                pause();
+                stopping = "stop";
+              }
+              // Abort should not be dispatched without support of "stopContinuousRecognitionAsync".
+              // But for defensive purpose, we make sure "stopContinuousRecognitionAsync" is available before we can call.
+              if (abort && this.recognizer.stopContinuousRecognitionAsync)
+                await $87729c1c3277125d$var$cognitiveServicesAsyncToPromise(
+                  this.recognizer.stopContinuousRecognitionAsync.bind(
+                    this.recognizer,
+                  ),
+                )();
+            } else if (audioSourceReady) {
+              this.processSendEvent("audiostart");
+              audioStarted = true;
+            } else if (firstAudibleChunk) {
+              this.processSendEvent("soundstart");
+              soundStarted = true;
+            } else if (audioSourceOff) {
+              // Looks like we don't need this line and all the tests are still working.
+              // Guessing probably stopping is already truthy.
+              // stopping = true;
+              speechStarted && this.processSendEvent("speechend");
+              soundStarted && this.processSendEvent("soundend");
+              audioStarted && this.processSendEvent("audioend");
+              audioStarted = soundStarted = speechStarted = false;
+              break;
+            } else if (stopping !== "abort") {
+              if (
+                recognized &&
+                recognized.result &&
+                recognized.result.reason ===
+                  $8zHUo$microsoftcognitiveservicesspeechsdk.ResultReason
+                    .NoMatch
+              )
+                finalEvent = {
+                  data: "no-speech",
+                  type: "error",
                 };
-                this.recognizer.sessionStopped = (_, { sessionId: sessionId })=>{
-                    // "sessionStopped" is never fired, probably because we are using startContinuousRecognitionAsync instead of recognizeOnceAsync.
-                    queue.push({
-                        sessionStopped: {
-                            sessionId: sessionId
-                        }
-                    });
-                };
-                this.recognizer.speechStartDetected = (_, { offset: offset, sessionId: sessionId })=>{
-                    queue.push({
-                        speechStartDetected: {
-                            offset: offset,
-                            sessionId: sessionId
-                        }
-                    });
-                };
-                this.recognizer.speechEndDetected = (_, { sessionId: sessionId })=>{
-                    // "speechEndDetected" is never fired, probably because we are using startContinuousRecognitionAsync instead of recognizeOnceAsync.
-                    // Update: "speechEndDetected" is fired for DLSpeech.listenOnceAsync()
-                    queue.push({
-                        speechEndDetected: {
-                            sessionId: sessionId
-                        }
-                    });
-                };
-                await $87729c1c3277125d$var$cognitiveServicesAsyncToPromise(this.recognizer.startContinuousRecognitionAsync.bind(this.recognizer))();
-                this.abort = ()=>queue.push({
-                        abort: {}
-                    });
-                this.stop = ()=>queue.push({
-                        stop: {}
-                    });
-                let audioStarted = false;
-                let finalEvent = null;
-                let finalizedResults = [];
-                for(let loop = 0; !stopping || audioStarted; loop++){
-                    const event = await queue.shift();
-                    const { abort: abort, audioSourceOff: audioSourceOff, audioSourceReady: audioSourceReady, canceled: canceled, firstAudibleChunk: firstAudibleChunk, recognized: recognized, recognizing: recognizing, stop: stop } = event;
-                    const errorMessage = canceled && canceled.errorDetails;
-                    if (/Permission\sdenied/u.test(errorMessage || "")) {
-                        // If microphone is not allowed, we should not emit "start" event.
-                        finalEvent = {
-                            data: "not-allowed",
-                            type: "error"
-                        };
-                        break;
-                    }
-                    if (!loop) this.processSendEvent("start");
-                    if (errorMessage) {
-                        if (/1006/u.test(errorMessage)) {
-                            if (!audioStarted) {
-                                this.processSendEvent("audiostart");
-                                this.processSendEvent("audioend");
-                            }
-                            finalEvent = {
-                                data: "network",
-                                type: "error"
-                            };
-                        } else finalEvent = {
-                            data: "unknown",
-                            type: "error"
-                        };
-                        break;
-                    } else if (abort || stop) {
-                        if (abort) {
-                            finalEvent = {
-                                type: "abort"
-                            };
-                            // If we are aborting, we will ignore lingering recognizing/recognized events. But if we are stopping, we need them.
-                            stopping = "abort";
-                        } else {
-                            // When we pause, we will send { isEnd: true }, Speech Services will send us "recognized" event.
-                            pause();
-                            stopping = "stop";
-                        }
-                        // Abort should not be dispatched without support of "stopContinuousRecognitionAsync".
-                        // But for defensive purpose, we make sure "stopContinuousRecognitionAsync" is available before we can call.
-                        if (abort && this.recognizer.stopContinuousRecognitionAsync) await $87729c1c3277125d$var$cognitiveServicesAsyncToPromise(this.recognizer.stopContinuousRecognitionAsync.bind(this.recognizer))();
-                    } else if (audioSourceReady) {
-                        this.processSendEvent("audiostart");
-                        audioStarted = true;
-                    } else if (firstAudibleChunk) {
-                        this.processSendEvent("soundstart");
-                        soundStarted = true;
-                    } else if (audioSourceOff) {
-                        // Looks like we don't need this line and all the tests are still working.
-                        // Guessing probably stopping is already truthy.
-                        // stopping = true;
-                        speechStarted && this.processSendEvent("speechend");
-                        soundStarted && this.processSendEvent("soundend");
-                        audioStarted && this.processSendEvent("audioend");
-                        audioStarted = soundStarted = speechStarted = false;
-                        break;
-                    } else if (stopping !== "abort") {
-                        if (recognized && recognized.result && recognized.result.reason === $8zHUo$microsoftcognitiveservicesspeechsdk.ResultReason.NoMatch) finalEvent = {
-                            data: "no-speech",
-                            type: "error"
-                        };
-                        else if (recognized || recognizing) {
-                            if (!audioStarted) {
-                                // Unconfirmed prevention of quirks
-                                this.processSendEvent("audiostart");
-                                audioStarted = true;
-                            }
-                            if (!soundStarted) {
-                                this.processSendEvent("soundstart");
-                                soundStarted = true;
-                            }
-                            if (!speechStarted) {
-                                this.processSendEvent("speechstart");
-                                speechStarted = true;
-                            }
-                            if (recognized) {
-                                const result = (0, $4e800713d8cfc243$export$2e2bcd8739ae039)(recognized.result, {
-                                    maxAlternatives: this.maxAlternatives,
-                                    textNormalization: this.textNormalization
-                                });
-                                const recognizable = !!result[0].transcript;
-                                if (recognizable) {
-                                    finalizedResults = [
-                                        ...finalizedResults,
-                                        ...result
-                                    ];
-                                    if (this._interimResults) {
-                                        this.processSendEvent("result", {
-                                            results: finalizedResults
-                                        });
-                                        this._passive && this.processSendEvent("passiveresult", {
-                                            results: result
-                                        });
-                                    }
-                                }
-                                // If it is continuous, we just sent the finalized results. So we don't need to send it again after "audioend" event.
-                                if (this._continuous && recognizable) finalEvent = null;
-                                else this._interimResults && (finalEvent = {
-                                    data: {
-                                        results: finalizedResults
-                                    },
-                                    type: "result"
-                                });
-                                if (!this._continuous && this.recognizer.stopContinuousRecognitionAsync) await $87729c1c3277125d$var$cognitiveServicesAsyncToPromise(this.recognizer.stopContinuousRecognitionAsync.bind(this.recognizer))();
-                                // If event order can be loosened, we can send the recognized event as soon as we receive it.
-                                // 1. If it is not recognizable (no-speech), we should send an "error" event just before "end" event. We will not loosen "error" events.
-                                if (this.looseEvents && finalEvent && recognizable) {
-                                    this.processSendEvent(finalEvent.type, finalEvent.data);
-                                    finalEvent = null;
-                                }
-                            } else if (recognizing) {
-                                const result = (0, $4e800713d8cfc243$export$2e2bcd8739ae039)(recognizing.result, {
-                                    maxAlternatives: this.maxAlternatives,
-                                    textNormalization: this.textNormalization
-                                });
-                                if (this._passive && this._wakeWords && this._wakeWords.length > 0) {
-                                    // Test wake words if some are present
-                                    if (result && Array.isArray(result) && result.length > 0 && result[0].transcript) {
-                                        const { transcript: transcript } = {
-                                            ...result[0]
-                                        };
-                                        this._wakeWords.forEach((wakeWord)=>{
-                                            if (transcript.toLowerCase().includes(wakeWord.toLowerCase())) this.processSendEvent("wakeup");
-                                        });
-                                    }
-                                    this._interimResults && this.processSendEvent("passiveresult", {
-                                        results: result
-                                    });
-                                }
-                                this._interimResults && this.processSendEvent("result", {
-                                    results: [
-                                        ...finalizedResults,
-                                        ...result
-                                    ]
-                                });
-                            }
-                        }
-                    }
+              else if (recognized || recognizing) {
+                if (!audioStarted) {
+                  // Unconfirmed prevention of quirks
+                  this.processSendEvent("audiostart");
+                  audioStarted = true;
+                }
+                if (!soundStarted) {
+                  this.processSendEvent("soundstart");
+                  soundStarted = true;
+                }
+                if (!speechStarted) {
+                  this.processSendEvent("speechstart");
+                  speechStarted = true;
                 }
-                speechStarted && this.processSendEvent("speechend");
-                soundStarted && this.processSendEvent("soundend");
-                audioStarted && this.processSendEvent("audioend");
-                if (finalEvent) {
-                    if (finalEvent.type === "result" && (typeof finalEvent.data === "string" || !finalEvent.data?.results || !finalEvent.data.results.length)) finalEvent = {
-                        data: "no-speech",
-                        type: "error"
-                    };
+                if (recognized) {
+                  const result = (0, $4e800713d8cfc243$export$2e2bcd8739ae039)(
+                    recognized.result,
+                    {
+                      maxAlternatives: this.maxAlternatives,
+                      textNormalization: this.textNormalization,
+                    },
+                  );
+                  const recognizable = !!result[0].transcript;
+                  if (recognizable) {
+                    finalizedResults = [...finalizedResults, ...result];
+                    if (this._interimResults) {
+                      this.processSendEvent("result", {
+                        results: finalizedResults,
+                      });
+                      this._passive &&
+                        this.processSendEvent("passiveresult", {
+                          results: result,
+                        });
+                    }
+                  }
+                  // If it is continuous, we just sent the finalized results. So we don't need to send it again after "audioend" event.
+                  if (this._continuous && recognizable) finalEvent = null;
+                  else
+                    this._interimResults &&
+                      (finalEvent = {
+                        data: {
+                          results: finalizedResults,
+                        },
+                        type: "result",
+                      });
+                  if (
+                    !this._continuous &&
+                    this.recognizer.stopContinuousRecognitionAsync
+                  )
+                    await $87729c1c3277125d$var$cognitiveServicesAsyncToPromise(
+                      this.recognizer.stopContinuousRecognitionAsync.bind(
+                        this.recognizer,
+                      ),
+                    )();
+                  // If event order can be loosened, we can send the recognized event as soon as we receive it.
+                  // 1. If it is not recognizable (no-speech), we should send an "error" event just before "end" event. We will not loosen "error" events.
+                  if (this.looseEvents && finalEvent && recognizable) {
                     this.processSendEvent(finalEvent.type, finalEvent.data);
+                    finalEvent = null;
+                  }
+                } else if (recognizing) {
+                  const result = (0, $4e800713d8cfc243$export$2e2bcd8739ae039)(
+                    recognizing.result,
+                    {
+                      maxAlternatives: this.maxAlternatives,
+                      textNormalization: this.textNormalization,
+                    },
+                  );
+                  if (
+                    this._passive &&
+                    this._wakeWords &&
+                    this._wakeWords.length > 0
+                  ) {
+                    // Test wake words if some are present
+                    if (
+                      result &&
+                      Array.isArray(result) &&
+                      result.length > 0 &&
+                      result[0].transcript
+                    ) {
+                      const { transcript: transcript } = {
+                        ...result[0],
+                      };
+                      this._wakeWords.forEach((wakeWord) => {
+                        if (
+                          transcript
+                            .toLowerCase()
+                            .includes(wakeWord.toLowerCase())
+                        )
+                          this.processSendEvent("wakeup");
+                      });
+                    }
+                    this._interimResults &&
+                      this.processSendEvent("passiveresult", {
+                        results: result,
+                      });
+                  }
+                  this._interimResults &&
+                    this.processSendEvent("result", {
+                      results: [...finalizedResults, ...result],
+                    });
                 }
-                // Even though there is no "start" event emitted, we will still emit "end" event
-                // This is mainly for "microphone blocked" story.
-                this.processSendEvent("end");
-                detachAudioConfigEvent && detachAudioConfigEvent();
-            } catch (err) {
-                // Logging out the erorr because Speech SDK would fail silently.
-                console.error(err);
-                throw err;
-            } finally{
-                unprepare();
-                this.recognizer.close();
+              }
             }
+          }
+          speechStarted && this.processSendEvent("speechend");
+          soundStarted && this.processSendEvent("soundend");
+          audioStarted && this.processSendEvent("audioend");
+          if (finalEvent) {
+            if (
+              finalEvent.type === "result" &&
+              (typeof finalEvent.data === "string" ||
+                !finalEvent.data?.results ||
+                !finalEvent.data.results.length)
+            )
+              finalEvent = {
+                data: "no-speech",
+                type: "error",
+              };
+            this.processSendEvent(finalEvent.type, finalEvent.data);
+          }
+          // Even though there is no "start" event emitted, we will still emit "end" event
+          // This is mainly for "microphone blocked" story.
+          this.processSendEvent("end");
+          detachAudioConfigEvent && detachAudioConfigEvent();
+        } catch (err) {
+          // Logging out the erorr because Speech SDK would fail silently.
+          console.error(err);
+          throw err;
+        } finally {
+          unprepare();
+          this.recognizer.close();
         }
     }
+  }
 }
-const $87729c1c3277125d$export$7c06391dbd8298b0 = (options, data)=>{
-    return {
-        speechRecognition: new $87729c1c3277125d$export$7a5e8c1807e6b8ee(options, data),
-        SpeechGrammarList: $c213b3979e1323e8$export$2e2bcd8739ae039
-    };
+const $87729c1c3277125d$export$7c06391dbd8298b0 = (options, data) => {
+  return {
+    speechRecognition: new $87729c1c3277125d$export$7a5e8c1807e6b8ee(
+      options,
+      data,
+    ),
+    SpeechGrammarList: $c213b3979e1323e8$export$2e2bcd8739ae039,
+  };
 };
-var $87729c1c3277125d$export$2e2bcd8739ae039 = $87729c1c3277125d$export$7c06391dbd8298b0;
-
-
-
+var $87729c1c3277125d$export$2e2bcd8739ae039 =
+  $87729c1c3277125d$export$7c06391dbd8298b0;
 
 /* eslint no-empty: ["error", { "allowEmptyCatch": true }] */ class $fd1e2557ea351c52$var$SpeechSynthesisUtterance extends EventTarget {
-    constructor(text){
-        super();
-        this._lang = undefined;
-        this._pitch = 1;
-        this._rate = 1;
-        this._voice = undefined;
-        this._volume = 1;
-        this.text = text;
-    }
-    get lang() {
-        return this._lang;
-    }
-    set lang(value) {
-        this._lang = value;
-    }
-    get pitch() {
-        return this._pitch;
-    }
-    set pitch(value) {
-        this._pitch = value;
-    }
-    get rate() {
-        return this._rate;
-    }
-    set rate(value) {
-        this._rate = value;
-    }
-    get voice() {
-        return this._voice;
-    }
-    set voice(value) {
-        this._voice = value;
-    }
-    get volume() {
-        return this._volume;
-    }
-    set volume(value) {
-        this._volume = value;
-    }
-    onstart() {}
-    onend() {}
-    onerror(error) {
-        console.log(error);
-    }
-    onsynthesisstart() {}
-    onsynthesiscompleted() {}
-    onboundary(data) {
-        console.log(data);
-    }
-    onviseme(data) {
-        console.log(data);
-    }
-    onmark(data) {
-        console.log(data);
-    }
+  constructor(text) {
+    super();
+    this._lang = undefined;
+    this._pitch = 1;
+    this._rate = 1;
+    this._voice = undefined;
+    this._volume = 1;
+    this.text = text;
+  }
+  get lang() {
+    return this._lang;
+  }
+  set lang(value) {
+    this._lang = value;
+  }
+  get pitch() {
+    return this._pitch;
+  }
+  set pitch(value) {
+    this._pitch = value;
+  }
+  get rate() {
+    return this._rate;
+  }
+  set rate(value) {
+    this._rate = value;
+  }
+  get voice() {
+    return this._voice;
+  }
+  set voice(value) {
+    this._voice = value;
+  }
+  get volume() {
+    return this._volume;
+  }
+  set volume(value) {
+    this._volume = value;
+  }
+  onstart() {}
+  onend() {}
+  onerror(error) {
+    console.log(error);
+  }
+  onsynthesisstart() {}
+  onsynthesiscompleted() {}
+  onboundary(data) {
+    // console.log(data);
+  }
+  onviseme(data) {
+    // console.log(data);
+  }
+  onmark(data) {
+    // console.log(data);
+  }
 }
-var $fd1e2557ea351c52$export$2e2bcd8739ae039 = $fd1e2557ea351c52$var$SpeechSynthesisUtterance;
-
+var $fd1e2557ea351c52$export$2e2bcd8739ae039 =
+  $fd1e2557ea351c52$var$SpeechSynthesisUtterance;
 
 class $fd058ea71144ee3c$export$2e2bcd8739ae039 {
-    constructor({ gender: gender, lang: lang, voiceURI: voiceURI }){
-        this._default = false;
-        this._localService = false;
-        this._gender = gender;
-        this._lang = lang;
-        this._name = voiceURI;
-        this._voiceURI = voiceURI;
-    }
-    get default() {
-        return this._default;
-    }
-    get gender() {
-        return this._gender;
-    }
-    get lang() {
-        return this._lang;
-    }
-    get localService() {
-        return this._localService;
-    }
-    get name() {
-        return this._name;
-    }
-    get voiceURI() {
-        return this._voiceURI;
-    }
+  constructor({ gender: gender, lang: lang, voiceURI: voiceURI }) {
+    this._default = false;
+    this._localService = false;
+    this._gender = gender;
+    this._lang = lang;
+    this._name = voiceURI;
+    this._voiceURI = voiceURI;
+  }
+  get default() {
+    return this._default;
+  }
+  get gender() {
+    return this._gender;
+  }
+  get lang() {
+    return this._lang;
+  }
+  get localService() {
+    return this._localService;
+  }
+  get name() {
+    return this._name;
+  }
+  get voiceURI() {
+    return this._voiceURI;
+  }
 }
 
-
 class $0adf7fcdea541e7b$export$1268b12b5ca510be {
-    constructor(options){
-        this.speaking = false;
-        this.speakerAudioDestination = new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeakerAudioDestination();
-        this.audioConfig = undefined;
-        this.speechConfig = null;
-        this.queue = [];
-        this.canceled = false;
-        this.synth = null;
-        this.synthesizing = false;
-        // Function that returns an empty array of available voices
-        this.getVoices = ()=>{
-            return [];
-        };
-        this.onvoiceschanged = ()=>{
-            console.log("Voices changed");
-        };
-        // Extract parameters from options using the patchOptions function
-        const { audioContext: audioContext, fetchCredentials: fetchCredentials, ponyfill: ponyfill = {
-            // @ts-ignore
-            AudioContext: window.AudioContext || window.webkitAudioContext
-        }, speechSynthesisDeploymentId: speechSynthesisDeploymentId } = (0, $cec424af042710dc$export$2e2bcd8739ae039)(options);
-        // Check if the browser supports the Web Audio API, if not, return an empty object
-        if (!audioContext && !ponyfill.AudioContext) console.warn("web-speech-cognitive-services: This browser does not support Web Audio and it will not work with Cognitive Services Speech Services.");
-        else {
-            this.audioConfig = $8zHUo$microsoftcognitiveservicesspeechsdk.AudioConfig.fromSpeakerOutput(this.speakerAudioDestination);
-            // Init synthesizer
-            this.initSpeechSynthesizer(fetchCredentials, speechSynthesisDeploymentId);
-        }
-    }
-    mute() {
-        this.speakerAudioDestination && this.speakerAudioDestination.mute();
-    }
-    unmute() {
-        this.speakerAudioDestination && this.speakerAudioDestination.unmute();
-    }
-    getVolume() {
-        // eslint-disable-next-line no-magic-numbers
-        return this.speakerAudioDestination ? this.speakerAudioDestination.volume : -1;
-    }
-    setVolume(value) {
-        this.speakerAudioDestination && (this.speakerAudioDestination.volume = value);
+  constructor(options) {
+    this.speaking = false;
+    this.speakerAudioDestination =
+      new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeakerAudioDestination();
+    this.audioConfig = undefined;
+    this.speechConfig = null;
+    this.queue = [];
+    this.canceled = false;
+    this.synth = null;
+    this.synthesizing = false;
+    // Function that returns an empty array of available voices
+    this.getVoices = () => {
+      return [];
+    };
+    this.onvoiceschanged = () => {
+      // console.log("Voices changed");
+    };
+    // Extract parameters from options using the patchOptions function
+    const {
+      audioContext: audioContext,
+      fetchCredentials: fetchCredentials,
+      ponyfill: ponyfill = {
+        // @ts-ignore
+        AudioContext: window.AudioContext || window.webkitAudioContext,
+      },
+      speechSynthesisDeploymentId: speechSynthesisDeploymentId,
+    } = (0, $cec424af042710dc$export$2e2bcd8739ae039)(options);
+    // Check if the browser supports the Web Audio API, if not, return an empty object
+    if (!audioContext && !ponyfill.AudioContext)
+      console.warn(
+        "web-speech-cognitive-services: This browser does not support Web Audio and it will not work with Cognitive Services Speech Services.",
+      );
+    else {
+      this.audioConfig =
+        $8zHUo$microsoftcognitiveservicesspeechsdk.AudioConfig.fromSpeakerOutput(
+          this.speakerAudioDestination,
+        );
+      // Init synthesizer
+      this.initSpeechSynthesizer(fetchCredentials, speechSynthesisDeploymentId);
     }
-    // Asynchronous function that initializes the speech synthesizer class
-    async initSpeechSynthesizer(fetchCredentials, speechSynthesisDeploymentId) {
-        const { speechSynthesisHostname: speechSynthesisHostname, subscriptionKey: subscriptionKey, authorizationToken: authorizationToken, region: region } = await fetchCredentials();
-        if (!authorizationToken && !subscriptionKey) throw new Error("no subscription data : authorizationToken or subscriptionKey needed");
-        // Configure the synthesizer and audio
-        if (speechSynthesisDeploymentId) {
-            const hostname = speechSynthesisHostname || `${region}.customvoice.api.speech.microsoft.com`;
-            const url = `https://${encodeURI(hostname)}/api/texttospeech/v2.0/endpoints/${encodeURIComponent(speechSynthesisDeploymentId)}`;
-            this.speechConfig = $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromEndpoint(new URL(url), subscriptionKey);
-        } else if (speechSynthesisHostname) this.speechConfig = $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromHost(speechSynthesisHostname, subscriptionKey);
-        else this.speechConfig = authorizationToken ? $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromAuthorizationToken(authorizationToken, region) : $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromSubscription(subscriptionKey, region);
-        this.synth = new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechSynthesizer(this.speechConfig, this.audioConfig);
-        // Update available voices
-        this.updateVoices();
+  }
+  mute() {
+    this.speakerAudioDestination && this.speakerAudioDestination.mute();
+  }
+  unmute() {
+    this.speakerAudioDestination && this.speakerAudioDestination.unmute();
+  }
+  getVolume() {
+    // eslint-disable-next-line no-magic-numbers
+    return this.speakerAudioDestination
+      ? this.speakerAudioDestination.volume
+      : -1;
+  }
+  setVolume(value) {
+    this.speakerAudioDestination &&
+      (this.speakerAudioDestination.volume = value);
+  }
+  // Asynchronous function that initializes the speech synthesizer class
+  async initSpeechSynthesizer(fetchCredentials, speechSynthesisDeploymentId) {
+    const {
+      speechSynthesisHostname: speechSynthesisHostname,
+      subscriptionKey: subscriptionKey,
+      authorizationToken: authorizationToken,
+      region: region,
+    } = await fetchCredentials();
+    if (!authorizationToken && !subscriptionKey)
+      throw new Error(
+        "no subscription data : authorizationToken or subscriptionKey needed",
+      );
+    // Configure the synthesizer and audio
+    if (speechSynthesisDeploymentId) {
+      const hostname =
+        speechSynthesisHostname ||
+        `${region}.customvoice.api.speech.microsoft.com`;
+      const url = `https://${encodeURI(hostname)}/api/texttospeech/v2.0/endpoints/${encodeURIComponent(speechSynthesisDeploymentId)}`;
+      this.speechConfig =
+        $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromEndpoint(
+          new URL(url),
+          subscriptionKey,
+        );
+    } else if (speechSynthesisHostname)
+      this.speechConfig =
+        $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromHost(
+          speechSynthesisHostname,
+          subscriptionKey,
+        );
+    else
+      this.speechConfig = authorizationToken
+        ? $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromAuthorizationToken(
+            authorizationToken,
+            region,
+          )
+        : $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechConfig.fromSubscription(
+            subscriptionKey,
+            region,
+          );
+    this.synth =
+      new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechSynthesizer(
+        this.speechConfig,
+        this.audioConfig,
+      );
+    // Update available voices
+    this.updateVoices();
+  }
+  // Function to recreate the synthesizer
+  createSynthesizer(voice, stream) {
+    if (stream)
+      this.audioConfig =
+        $8zHUo$microsoftcognitiveservicesspeechsdk.AudioConfig.fromStreamOutput(
+          stream,
+        );
+    else {
+      this.speakerAudioDestination =
+        new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeakerAudioDestination();
+      this.audioConfig =
+        $8zHUo$microsoftcognitiveservicesspeechsdk.AudioConfig.fromSpeakerOutput(
+          this.speakerAudioDestination,
+        );
     }
-    // Function to recreate the synthesizer
-    createSynthesizer(voice, stream) {
-        if (stream) this.audioConfig = $8zHUo$microsoftcognitiveservicesspeechsdk.AudioConfig.fromStreamOutput(stream);
-        else {
-            this.speakerAudioDestination = new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeakerAudioDestination();
-            this.audioConfig = $8zHUo$microsoftcognitiveservicesspeechsdk.AudioConfig.fromSpeakerOutput(this.speakerAudioDestination);
-        }
-        if (this.speechConfig) {
-            if (voice) {
-                const tempSpeechConfig = this.speechConfig;
-                tempSpeechConfig.speechSynthesisVoiceName = voice;
-                this.synth = new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechSynthesizer(tempSpeechConfig, this.audioConfig);
-            } else this.synth = new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechSynthesizer(this.speechConfig, this.audioConfig);
-        }
+    if (this.speechConfig) {
+      if (voice) {
+        const tempSpeechConfig = this.speechConfig;
+        tempSpeechConfig.speechSynthesisVoiceName = voice;
+        this.synth =
+          new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechSynthesizer(
+            tempSpeechConfig,
+            this.audioConfig,
+          );
+      } else
+        this.synth =
+          new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechSynthesizer(
+            this.speechConfig,
+            this.audioConfig,
+          );
     }
-    // Cancel current synthesis
-    cancel() {
-        if (this.synthesizing) try {
-            // @ts-ignore
-            this.synth && !this.synth.privDisposed && this.synth.close();
-            this.queue = [];
-        } catch (e) {
-            console.warn(e);
-        }
-        else if (this.speaking) try {
-            this.speakerAudioDestination.pause();
-            this.speakerAudioDestination.onAudioEnd(this.speakerAudioDestination);
-            this.speakerAudioDestination.close();
-        } catch (e) {
-            console.warn(e);
-        }
+  }
+  // Cancel current synthesis
+  cancel() {
+    if (this.synthesizing)
+      try {
+        // @ts-ignore
+        this.synth && !this.synth.privDisposed && this.synth.close();
         this.queue = [];
-        this.canceled = true;
-    }
-    // Pause current synthesis
-    pause() {
-        if (this.speakerAudioDestination) this.speakerAudioDestination.pause();
-    }
-    // Resume current synthesis
-    resume() {
-        if (this.speakerAudioDestination && !this.canceled) this.speakerAudioDestination.resume();
-    }
-    /**
+      } catch (e) {
+        console.warn(e);
+      }
+    else if (this.speaking)
+      try {
+        this.speakerAudioDestination.pause();
+        this.speakerAudioDestination.onAudioEnd(this.speakerAudioDestination);
+        this.speakerAudioDestination.close();
+      } catch (e) {
+        console.warn(e);
+      }
+    this.queue = [];
+    this.canceled = true;
+  }
+  // Pause current synthesis
+  pause() {
+    if (this.speakerAudioDestination) this.speakerAudioDestination.pause();
+  }
+  // Resume current synthesis
+  resume() {
+    if (this.speakerAudioDestination && !this.canceled)
+      this.speakerAudioDestination.resume();
+  }
+  /**
    * Add events listeners to the events received by the synthesizer if callbakcs are given in the utterance
    * @param {SpeechSynthesisUtterance} utterance
    */ linkEventsCallbacks(utterance) {
-        if (this.synth) {
-            // Events callbacks
-            this.synth.synthesisStarted = ()=>{
-                utterance.onsynthesisstart && utterance.onsynthesisstart();
-                this.synthesizing = true;
-            };
-            this.synth.synthesisCompleted = ()=>{
-                utterance.onsynthesiscompleted && utterance.onsynthesiscompleted();
-                this.synthesizing = false;
-            };
-            this.synth.wordBoundary = (synth, e)=>{
-                !synth && console.warn("No synthesizer");
-                const data = {
-                    boundaryType: e.boundaryType,
-                    name: e.text,
-                    elapsedTime: e.audioOffset,
-                    duration: e.duration
-                };
-                utterance.onboundary && utterance.onboundary(data);
-            };
-            this.synth.visemeReceived = (synth, e)=>{
-                !synth && console.warn("No synthesizer");
-                const data = {
-                    boundaryType: "Viseme",
-                    name: `${e.visemeId}`,
-                    elapsedTime: e.audioOffset,
-                    duration: 0
-                };
-                utterance.onboundary && utterance.onboundary(data);
-                utterance.onviseme && utterance.onviseme(data);
-            };
-            this.synth.bookmarkReached = (synth, e)=>{
-                !synth && console.warn("No synthesizer");
-                const data = {
-                    boundaryType: "Mark",
-                    name: e.text,
-                    elapsedTime: e.audioOffset,
-                    duration: 0
-                };
-                utterance.onmark && utterance.onmark(data);
-            };
-        }
+    if (this.synth) {
+      // Events callbacks
+      this.synth.synthesisStarted = () => {
+        utterance.onsynthesisstart && utterance.onsynthesisstart();
+        this.synthesizing = true;
+      };
+      this.synth.synthesisCompleted = () => {
+        utterance.onsynthesiscompleted && utterance.onsynthesiscompleted();
+        this.synthesizing = false;
+      };
+      this.synth.wordBoundary = (synth, e) => {
+        !synth && console.warn("No synthesizer");
+        const data = {
+          boundaryType: e.boundaryType,
+          name: e.text,
+          elapsedTime: e.audioOffset,
+          duration: e.duration,
+        };
+        utterance.onboundary && utterance.onboundary(data);
+      };
+      this.synth.visemeReceived = (synth, e) => {
+        !synth && console.warn("No synthesizer");
+        const data = {
+          boundaryType: "Viseme",
+          name: `${e.visemeId}`,
+          elapsedTime: e.audioOffset,
+          duration: 0,
+        };
+        utterance.onboundary && utterance.onboundary(data);
+        utterance.onviseme && utterance.onviseme(data);
+      };
+      this.synth.bookmarkReached = (synth, e) => {
+        !synth && console.warn("No synthesizer");
+        const data = {
+          boundaryType: "Mark",
+          name: e.text,
+          elapsedTime: e.audioOffset,
+          duration: 0,
+        };
+        utterance.onmark && utterance.onmark(data);
+      };
     }
-    /**
+  }
+  /**
    * Launch synthesis and play sound with the speech synthesizer
-   * @param {SpeechSynthesisUtterance} utterance 
+   * @param {SpeechSynthesisUtterance} utterance
    */ speak(utterance, stream) {
-        // Test utterance
-        if (!(utterance instanceof (0, $fd1e2557ea351c52$export$2e2bcd8739ae039))) throw new Error("invalid utterance");
-        // Add the utterance to the queue
-        this.queue.push(utterance);
-        // Function to process the queued utterances
-        const processQueue = ()=>{
-            if (this.queue.length && !this.speaking) {
-                const currentUtterance = this.queue.shift(); // Get the next utterance from the queue
-                if (currentUtterance) {
-                    const isSSML = /<speak[\s\S]*?>/iu.test(currentUtterance.text);
-                    if (currentUtterance.voice && (currentUtterance.voice.voiceURI || currentUtterance.voice._name)) this.createSynthesizer(currentUtterance.voice.voiceURI || currentUtterance.voice._name, stream);
-                    else this.createSynthesizer(undefined, stream);
-                    // Set volume / mute status if present in the utterance parameters
-                    currentUtterance.volume && (this.speakerAudioDestination.volume = currentUtterance.volume);
-                    // SpeakerAudioDestination events callbacks
-                    this.speakerAudioDestination.onAudioStart = ()=>{
-                        this.speaking = true;
-                        currentUtterance.onstart && currentUtterance.onstart();
-                    };
-                    this.speakerAudioDestination.onAudioEnd = ()=>{
-                        this.speaking = false;
-                        currentUtterance.onend && currentUtterance.onend();
-                        processQueue(); // Process the next queued utterance after the current one has finished
-                    };
-                    this.linkEventsCallbacks(currentUtterance);
-                    return isSSML ? new Promise((reject)=>{
-                        this.synth && this.synth.speakSsmlAsync(currentUtterance.text, (result)=>{
-                            if (result) this.synth && this.synth.close();
-                            else reject(new Error("No synthesis result."));
-                        }, (error)=>{
-                            reject(new Error(`Synthesis failed : ${error}`));
-                        });
-                    }) : new Promise((reject)=>{
-                        this.synth && this.synth.speakTextAsync(currentUtterance.text, (result)=>{
-                            if (result) this.synth && this.synth.close();
-                            else reject(new Error("No synthesis result."));
-                        }, (error)=>{
-                            reject(new Error(`Synthesis failed : ${error}`));
-                        });
-                    });
-                }
-            }
-        };
-        processQueue(); // Start processing the queue
-        this.canceled = false; // Reset canceled state after processing the queue
-    }
-    /**
+    // Test utterance
+    if (!(utterance instanceof (0, $fd1e2557ea351c52$export$2e2bcd8739ae039)))
+      throw new Error("invalid utterance");
+    // Add the utterance to the queue
+    this.queue.push(utterance);
+    // Function to process the queued utterances
+    const processQueue = () => {
+      if (this.queue.length && !this.speaking) {
+        const currentUtterance = this.queue.shift(); // Get the next utterance from the queue
+        if (currentUtterance) {
+          const isSSML = /<speak[\s\S]*?>/iu.test(currentUtterance.text);
+          if (
+            currentUtterance.voice &&
+            (currentUtterance.voice.voiceURI || currentUtterance.voice._name)
+          )
+            this.createSynthesizer(
+              currentUtterance.voice.voiceURI || currentUtterance.voice._name,
+              stream,
+            );
+          else this.createSynthesizer(undefined, stream);
+          // Set volume / mute status if present in the utterance parameters
+          currentUtterance.volume &&
+            (this.speakerAudioDestination.volume = currentUtterance.volume);
+          // SpeakerAudioDestination events callbacks
+          this.speakerAudioDestination.onAudioStart = () => {
+            this.speaking = true;
+            currentUtterance.onstart && currentUtterance.onstart();
+          };
+          this.speakerAudioDestination.onAudioEnd = () => {
+            this.speaking = false;
+            currentUtterance.onend && currentUtterance.onend();
+            processQueue(); // Process the next queued utterance after the current one has finished
+          };
+          this.linkEventsCallbacks(currentUtterance);
+          return isSSML
+            ? new Promise((reject) => {
+                this.synth &&
+                  this.synth.speakSsmlAsync(
+                    currentUtterance.text,
+                    (result) => {
+                      if (result) this.synth && this.synth.close();
+                      else reject(new Error("No synthesis result."));
+                    },
+                    (error) => {
+                      reject(new Error(`Synthesis failed : ${error}`));
+                    },
+                  );
+              })
+            : new Promise((reject) => {
+                this.synth &&
+                  this.synth.speakTextAsync(
+                    currentUtterance.text,
+                    (result) => {
+                      if (result) this.synth && this.synth.close();
+                      else reject(new Error("No synthesis result."));
+                    },
+                    (error) => {
+                      reject(new Error(`Synthesis failed : ${error}`));
+                    },
+                  );
+              });
+        }
+      }
+    };
+    processQueue(); // Start processing the queue
+    this.canceled = false; // Reset canceled state after processing the queue
+  }
+  /**
    * Launch synthesis without sound being played and call callback function with an ArrayBuffer after synthesis finished, containing the sound data
-   * @param {SpeechSynthesisUtterance} utterance 
-   * @param {Function} callback 
+   * @param {SpeechSynthesisUtterance} utterance
+   * @param {Function} callback
    */ synthesizeAndGetArrayData(utterance, callback) {
-        // Test utterance
-        if (!(utterance instanceof (0, $fd1e2557ea351c52$export$2e2bcd8739ae039))) throw new Error("invalid utterance");
-        const isSSML = /<speak[\s\S]*?>/iu.test(utterance.text);
-        if (this.speechConfig) {
-            if (utterance.voice && (utterance.voice.voiceURI || utterance.voice._name)) {
-                const tempSpeechConfig = this.speechConfig;
-                tempSpeechConfig.speechSynthesisVoiceName = utterance.voice.voiceURI || utterance.voice._name;
-                // @ts-ignore
-                this.synth = new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechSynthesizer(tempSpeechConfig, null);
-            } else // @ts-ignore
-            this.synth = new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechSynthesizer(this.speechConfig, null);
-        }
-        this.linkEventsCallbacks(utterance);
-        try {
-            isSSML ? this.synth && this.synth.speakSsmlAsync(utterance.text, (result)=>{
-                if (result && result.audioData) {
-                    callback(result.audioData);
-                    this.synth && this.synth.close();
-                } else callback(null);
-            }, (error)=>{
-                console.error(error);
-                callback(null);
-            }) : this.synth && this.synth.speakTextAsync(utterance.text, (result)=>{
-                if (result && result.audioData) {
-                    callback(result.audioData);
-                    this.synth && this.synth.close();
-                } else callback(null);
-            }, (error)=>{
-                console.error(error);
-                callback(null);
-            });
-        } catch (error) {
-            console.error(error);
-        }
+    // Test utterance
+    if (!(utterance instanceof (0, $fd1e2557ea351c52$export$2e2bcd8739ae039)))
+      throw new Error("invalid utterance");
+    const isSSML = /<speak[\s\S]*?>/iu.test(utterance.text);
+    if (this.speechConfig) {
+      if (
+        utterance.voice &&
+        (utterance.voice.voiceURI || utterance.voice._name)
+      ) {
+        const tempSpeechConfig = this.speechConfig;
+        tempSpeechConfig.speechSynthesisVoiceName =
+          utterance.voice.voiceURI || utterance.voice._name;
+        // @ts-ignore
+        this.synth =
+          new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechSynthesizer(
+            tempSpeechConfig,
+            null,
+          );
+      } // @ts-ignore
+      else
+        this.synth =
+          new $8zHUo$microsoftcognitiveservicesspeechsdk.SpeechSynthesizer(
+            this.speechConfig,
+            null,
+          );
     }
-    // Asynchronous function that updates available voices
-    async updateVoices() {
-        const voicesResult = this.synth ? await this.synth.getVoicesAsync() : null;
-        const voices = voicesResult?.voices;
-        if (Array.isArray(voices)) {
-            const formattedVoices = voices.map((voice)=>new (0, $fd058ea71144ee3c$export$2e2bcd8739ae039)({
-                    // eslint-disable-next-line no-magic-numbers
-                    gender: voice.gender === 1 ? "Female" : voice.gender === 2 ? "Male" : "Undefined",
-                    lang: voice.locale,
-                    voiceURI: voice.name
-                }));
-            this.getVoices = ()=>formattedVoices;
-        } else console.warn("Failed to retrieve voices. 'voices' is not an array.");
-        // Call 'onvoiceschanged' callback to notify the voices update
-        this.onvoiceschanged();
+    this.linkEventsCallbacks(utterance);
+    try {
+      isSSML
+        ? this.synth &&
+          this.synth.speakSsmlAsync(
+            utterance.text,
+            (result) => {
+              if (result && result.audioData) {
+                callback(result.audioData);
+                this.synth && this.synth.close();
+              } else callback(null);
+            },
+            (error) => {
+              console.error(error);
+              callback(null);
+            },
+          )
+        : this.synth &&
+          this.synth.speakTextAsync(
+            utterance.text,
+            (result) => {
+              if (result && result.audioData) {
+                callback(result.audioData);
+                this.synth && this.synth.close();
+              } else callback(null);
+            },
+            (error) => {
+              console.error(error);
+              callback(null);
+            },
+          );
+    } catch (error) {
+      console.error(error);
     }
+  }
+  // Asynchronous function that updates available voices
+  async updateVoices() {
+    const voicesResult = this.synth ? await this.synth.getVoicesAsync() : null;
+    const voices = voicesResult?.voices;
+    if (Array.isArray(voices)) {
+      const formattedVoices = voices.map(
+        (voice) =>
+          new (0, $fd058ea71144ee3c$export$2e2bcd8739ae039)({
+            // eslint-disable-next-line no-magic-numbers
+            gender:
+              voice.gender === 1
+                ? "Female"
+                : voice.gender === 2
+                  ? "Male"
+                  : "Undefined",
+            lang: voice.locale,
+            voiceURI: voice.name,
+          }),
+      );
+      this.getVoices = () => formattedVoices;
+    } else console.warn("Failed to retrieve voices. 'voices' is not an array.");
+    // Call 'onvoiceschanged' callback to notify the voices update
+    this.onvoiceschanged();
+  }
 }
-const $0adf7fcdea541e7b$export$cf962f8b61871d5c = (options)=>{
-    return {
-        speechSynthesis: new $0adf7fcdea541e7b$export$1268b12b5ca510be(options),
-        SpeechSynthesisUtterance: $fd1e2557ea351c52$export$2e2bcd8739ae039
-    };
+const $0adf7fcdea541e7b$export$cf962f8b61871d5c = (options) => {
+  return {
+    speechSynthesis: new $0adf7fcdea541e7b$export$1268b12b5ca510be(options),
+    SpeechSynthesisUtterance: $fd1e2557ea351c52$export$2e2bcd8739ae039,
+  };
 };
-var $0adf7fcdea541e7b$export$2e2bcd8739ae039 = $0adf7fcdea541e7b$export$cf962f8b61871d5c;
+var $0adf7fcdea541e7b$export$2e2bcd8739ae039 =
+  $0adf7fcdea541e7b$export$cf962f8b61871d5c;
 
-
-const $3e54160c801e3614$var$TOKEN_URL_TEMPLATE = "https://{region}.api.cognitive.microsoft.com/sts/v1.0/issueToken";
-async function $3e54160c801e3614$export$2e2bcd8739ae039({ region: region, subscriptionKey: subscriptionKey }) {
-    if (!region || !subscriptionKey) throw new Error("Region AND subscription must be provided");
-    else {
-        const res = await fetch($3e54160c801e3614$var$TOKEN_URL_TEMPLATE.replace(/\{region\}/u, region), {
-            headers: {
-                "Ocp-Apim-Subscription-Key": subscriptionKey
-            },
-            method: "POST"
-        });
-        if (!res.ok) throw new Error(`Failed to fetch authorization token, server returned ${res.status}`);
-        return res.text();
-    }
+const $3e54160c801e3614$var$TOKEN_URL_TEMPLATE =
+  "https://{region}.api.cognitive.microsoft.com/sts/v1.0/issueToken";
+async function $3e54160c801e3614$export$2e2bcd8739ae039({
+  region: region,
+  subscriptionKey: subscriptionKey,
+}) {
+  if (!region || !subscriptionKey)
+    throw new Error("Region AND subscription must be provided");
+  else {
+    const res = await fetch(
+      $3e54160c801e3614$var$TOKEN_URL_TEMPLATE.replace(/\{region\}/u, region),
+      {
+        headers: {
+          "Ocp-Apim-Subscription-Key": subscriptionKey,
+        },
+        method: "POST",
+      },
+    );
+    if (!res.ok)
+      throw new Error(
+        `Failed to fetch authorization token, server returned ${res.status}`,
+      );
+    return res.text();
+  }
 }
 
-
-
-
 function $882b6d93070905b3$export$2e2bcd8739ae039(options, recognitionData) {
-    return {
-        ...(0, $87729c1c3277125d$export$7c06391dbd8298b0)(options, recognitionData),
-        ...(0, $0adf7fcdea541e7b$export$cf962f8b61871d5c)(options)
-    };
+  return {
+    ...(0, $87729c1c3277125d$export$7c06391dbd8298b0)(options, recognitionData),
+    ...(0, $0adf7fcdea541e7b$export$cf962f8b61871d5c)(options),
+  };
 }
 
-
 //# sourceMappingURL=index.js.map
